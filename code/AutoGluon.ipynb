{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cea59b28",
   "metadata": {
    "id": "ea7eb7ff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc, random\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Custom library\n",
    "# 시드 고정 함수\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "TOTAL_THRES = 300 # 구매액 임계값\n",
    "SEED = 42 # 랜덤 시드\n",
    "seed_everything(SEED) # 시드 고정\n",
    "\n",
    "data_dir = '../input/train.csv' # os.environ['SM_CHANNEL_TRAIN']\n",
    "model_dir = '../model' # os.environ['SM_MODEL_DIR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3323426b",
   "metadata": {
    "id": "bf703c94"
   },
   "outputs": [],
   "source": [
    "def feature_preprocessing(train, test, features, do_imputing=True):\n",
    "    x_tr = train.copy()\n",
    "    x_te = test.copy()\n",
    "    \n",
    "    # 범주형 피처 이름을 저장할 변수\n",
    "    cate_cols = []\n",
    "\n",
    "    # 레이블 인코딩\n",
    "    for f in features:\n",
    "        if x_tr[f].dtype.name == 'object': # 데이터 타입이 object(str)이면 레이블 인코딩\n",
    "            cate_cols.append(f)\n",
    "            le = LabelEncoder()\n",
    "            # train + test 데이터를 합쳐서 레이블 인코딩 함수에 fit\n",
    "            le.fit(list(x_tr[f].values) + list(x_te[f].values))\n",
    "            \n",
    "            # train 데이터 레이블 인코딩 변환 수행\n",
    "            x_tr[f] = le.transform(list(x_tr[f].values))\n",
    "            \n",
    "            # test 데이터 레이블 인코딩 변환 수행\n",
    "            x_te[f] = le.transform(list(x_te[f].values))\n",
    "\n",
    "    print('categorical feature:', cate_cols)\n",
    "\n",
    "    if do_imputing:\n",
    "        # 중위값으로 결측치 채우기\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        x_tr[features] = imputer.fit_transform(x_tr[features])\n",
    "        x_te[features] = imputer.transform(x_te[features])\n",
    "    \n",
    "    return x_tr, x_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d901f66",
   "metadata": {
    "id": "9f7f9c13"
   },
   "outputs": [],
   "source": [
    "def feature_engineering1(df, year_month):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    prev_ym = prev_ym.strftime('%Y-%m')\n",
    "    \n",
    "    # train, test 데이터 선택\n",
    "    train = df[df['order_date'] < prev_ym]\n",
    "    test = df[df['order_date'] < year_month]\n",
    "    \n",
    "    # train, test 레이블 데이터 생성\n",
    "    train_label = generate_label(df, prev_ym)[['customer_id','year_month','label']]\n",
    "    test_label = generate_label(df, year_month)[['customer_id','year_month','label']]\n",
    "    \n",
    "    # group by aggregation 함수 선언\n",
    "    agg_func = ['mean','max','min','sum','count','std','skew']\n",
    "    all_train_data = pd.DataFrame()\n",
    "    \n",
    "    for i, tr_ym in enumerate(train_label['year_month'].unique()):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        train_agg = train.loc[train['order_date'] < tr_ym].groupby(['customer_id']).agg(agg_func)\n",
    "\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for col in train_agg.columns.levels[0]:\n",
    "            for stat in train_agg.columns.levels[1]:\n",
    "                new_cols.append(f'{col}-{stat}')\n",
    "\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace = True)\n",
    "        \n",
    "        train_agg['year_month'] = tr_ym\n",
    "        \n",
    "        all_train_data = all_train_data.append(train_agg)\n",
    "    \n",
    "    all_train_data = train_label.merge(all_train_data, on=['customer_id', 'year_month'], how='left')\n",
    "    features = all_train_data.drop(columns=['customer_id', 'label', 'year_month']).columns\n",
    "    \n",
    "    # group by aggretation 함수로 test 데이터 피처 생성\n",
    "    test_agg = test.groupby(['customer_id']).agg(agg_func)\n",
    "    test_agg.columns = new_cols\n",
    "    \n",
    "    test_data = test_label.merge(test_agg, on=['customer_id'], how='left')\n",
    "\n",
    "    # train, test 데이터 전처리\n",
    "    x_tr, x_te = feature_preprocessing(all_train_data, test_data, features)\n",
    "    \n",
    "    print('x_tr.shape', x_tr.shape, ', x_te.shape', x_te.shape)\n",
    "    \n",
    "    return x_tr, x_te, all_train_data['label'], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8355a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_trend(df, year_month):\n",
    "    df = df.copy()\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    # train과 test 데이터 생성\n",
    "    train = df[df['order_date'] < prev_ym]  # 2009-12부터 2011-10 데이터 추출\n",
    "    test = df[df['order_date'] < year_month]  # 2009-12부터 2011-11 데이터 추출\n",
    "    train_window_ym = []\n",
    "    test_window_ym = []\n",
    "    for month_back in [1, 2, 3, 5, 7, 12, 20, 23]:  # 1개월, 2개월, ... 20개월, 23개월 전 year_month 파악\n",
    "        train_window_ym.append((prev_ym - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'))\n",
    "        test_window_ym.append((d - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'))\n",
    "    # aggregation 함수 선언\n",
    "    agg_func = ['max', 'min', 'sum', 'mean', 'count', 'std', 'skew']\n",
    "    # group by aggregation with Dictionary\n",
    "    agg_dict = {\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "    }\n",
    "    # general statistics for train data with time series trend\n",
    "    for i, tr_ym in enumerate(train_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        train_agg = train.loc[train['year_month'] >= tr_ym].groupby(['customer_id']).agg(\n",
    "            agg_dict)  # 해당 year_month 이후부터 모든 데이터에 대한 aggregation을 실시\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in train_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-{i}')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            train_data = train_agg\n",
    "        else:\n",
    "            train_data = train_data.merge(train_agg, on=['customer_id'], how='right')\n",
    "    # general statistics for test data with time series trend\n",
    "    for i, tr_ym in enumerate(test_window_ym):\n",
    "        # group by aggretation 함수로 test 데이터 피처 생성\n",
    "        test_agg = test.loc[test['year_month'] >= tr_ym].groupby(['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in test_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-{i}')\n",
    "        test_agg.columns = new_cols\n",
    "        test_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            test_data = test_agg\n",
    "        else:\n",
    "            test_data = test_data.merge(test_agg, on=['customer_id'], how='right')\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def add_seasonality(df, year_month):\n",
    "    df = df.copy()\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    # train과 test 데이터 생성\n",
    "    train = df[df['order_date'] < prev_ym]  # 2009-12부터 2011-10 데이터 추출\n",
    "    test = df[df['order_date'] < year_month]  # 2009-12부터 2011-11 데이터 추출\n",
    "    train_window_ym = []\n",
    "    test_window_ym = []\n",
    "    for month_back in [1, 6, 12, 18]:  # 각 주기성을 파악하고 싶은 구간을 생성\n",
    "        train_window_ym.append(\n",
    "            (\n",
    "                (prev_ym - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'),\n",
    "                (prev_ym - dateutil.relativedelta.relativedelta(months=month_back + 2)).strftime('%Y-%m')\n",
    "            # 1~3, 6~8, 12~14, 18~20 Pair를 만들어준다\n",
    "            )\n",
    "        )\n",
    "        test_window_ym.append(\n",
    "            (\n",
    "                (d - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'),\n",
    "                (d - dateutil.relativedelta.relativedelta(months=month_back + 2)).strftime('%Y-%m')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # aggregation 함수 선언\n",
    "    agg_func = ['max', 'min', 'sum', 'mean', 'count', 'std', 'skew']\n",
    "    # group by aggregation with Dictionary\n",
    "    agg_dict = {\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "    }\n",
    "    # seasonality for train data with time series\n",
    "    for i, (tr_ym, tr_ym_3) in enumerate(train_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        # 구간 사이에 존재하는 월들에 대해서 aggregation을 진행\n",
    "        train_agg = train.loc[(train['year_month'] >= tr_ym_3) & (train['year_month'] <= tr_ym)].groupby(\n",
    "            ['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in train_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-season{i}')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            train_data = train_agg\n",
    "        else:\n",
    "            train_data = train_data.merge(train_agg, on=['customer_id'], how='right')\n",
    "    # seasonality for test data with time series\n",
    "    for i, (tr_ym, tr_ym_3) in enumerate(test_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        test_agg = test.loc[(test['year_month'] >= tr_ym_3) & (test['year_month'] <= tr_ym)].groupby(\n",
    "            ['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in test_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-season{i}')\n",
    "        test_agg.columns = new_cols\n",
    "        test_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            test_data = test_agg\n",
    "        else:\n",
    "            test_data = test_data.merge(test_agg, on=['customer_id'], how='right')\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering2(df, year_month):\n",
    "    df = df.copy()\n",
    "\n",
    "    # customer_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_cust_id'] = df.groupby(['customer_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_cust_id'] = df.groupby(['customer_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_cust_id'] = df.groupby(['customer_id'])['price'].cumsum()\n",
    "\n",
    "    # product_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_prod_id'] = df.groupby(['product_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_prod_id'] = df.groupby(['product_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_prod_id'] = df.groupby(['product_id'])['price'].cumsum()\n",
    "\n",
    "    # order_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_order_id'] = df.groupby(['order_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_order_id'] = df.groupby(['order_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_order_id'] = df.groupby(['order_id'])['price'].cumsum()\n",
    "\n",
    "    # oredr_ts\n",
    "    df['order_ts'] = df['order_date'].astype(np.int64)//1e9\n",
    "    df['order_ts_diff'] = df.groupby(['customer_id'])['order_ts'].diff()\n",
    "    df['quantity_diff'] = df.groupby(['customer_id'])['quantity'].diff()\n",
    "    df['price_diff'] = df.groupby(['customer_id'])['price'].diff()\n",
    "    df['total_diff'] = df.groupby(['customer_id'])['total'].diff()\n",
    "\n",
    "    # mode\n",
    "    df['month-mode'] = df['order_date'].dt.month\n",
    "    df['year_month-mode'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "    # oredr_ts_plus ===\n",
    "    df['order_ts_plus'] = df[df['total'] > 0]['order_date'].astype(np.int64) // 1e9\n",
    "    df['order_ts_plus_diff'] = df[df['total'] > 0].groupby(['customer_id'])['order_ts'].diff()\n",
    "    df['order_ts_plus'] = df['order_ts_plus'].fillna(0)\n",
    "    df['order_ts_plus_diff'] = df['order_ts_plus_diff'].fillna(0)\n",
    "    # df[~(df.order_id.str.contains('C'))].groupby(['customer_id'])['order_date'].last().astype(np.int64) // 1e9\n",
    "\n",
    "    # ================================================================================================\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    prev_ym = prev_ym.strftime('%Y-%m')\n",
    "\n",
    "    # train, test 데이터 선택\n",
    "    train = df[df['order_date'] < prev_ym]\n",
    "    test = df[df['order_date'] < year_month]\n",
    "\n",
    "    # train, test 레이블 데이터 생성\n",
    "    train_label = generate_label(df, prev_ym)[['customer_id', 'year_month', 'label']]\n",
    "    test_label = generate_label(df, year_month)[['customer_id', 'year_month', 'label']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 연월 피처 생성\n",
    "    target = datetime.datetime.strptime('2011-11', \"%Y-%m\")  # 타겟 연월\n",
    "    prev = target - dateutil.relativedelta.relativedelta(years=1)  # 전년 연월\n",
    "    prev = prev.strftime('%Y-%m')  # 문자열로 변환\n",
    "    groupby = train.groupby(['customer_id', 'year_month-mode'])['total'].sum()  # 고객별, 월별 total 합\n",
    "    groupby = groupby.unstack()  # 월별을 컬럼으로 변환\n",
    "    prev_pprev_total = groupby.loc[:, [prev]]  # 전년, 전전년 데이터만 추출\n",
    "    prev_pprev_total = prev_pprev_total.fillna(0)\n",
    "\n",
    "    train_1224 = (prev_pprev_total['2010-11']) / 2\n",
    "\n",
    "\n",
    "    target = datetime.datetime.strptime('2011-12', \"%Y-%m\")  # 타겟 연월\n",
    "    prev = target - dateutil.relativedelta.relativedelta(years=1)  # 전년 연월\n",
    "    pprev = prev - dateutil.relativedelta.relativedelta(years=1)  # 전전년 연월\n",
    "    prev, pprev = prev.strftime('%Y-%m'), pprev.strftime('%Y-%m')  # 문자열로 변환\n",
    "    groupby = test.groupby(['customer_id', 'year_month-mode'])['total'].sum()  # 고객별, 월별 total 합\n",
    "    groupby = groupby.unstack()  # 월별을 컬럼으로 변환\n",
    "    prev_pprev_total = groupby.loc[:, [prev, pprev]]  # 전년, 전전년 데이터만 추출\n",
    "    prev_pprev_total = prev_pprev_total.fillna(0)\n",
    "\n",
    "    test_1224 = (prev_pprev_total['2010-12'] + prev_pprev_total['2009-12']) / 2\n",
    "\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    # lambda 식\n",
    "    mode_f = lambda x: x.value_counts().index[0]\n",
    "\n",
    "    # group by aggregation 함수 선언\n",
    "    agg_func = ['mean', 'max', 'min', 'sum', 'count', 'std', 'skew']\n",
    "    # agg_func = ['mean', 'max'] # , 'min', 'sum', 'count', 'std', 'skew']\n",
    "    agg_dict = {\n",
    "        'order_ts': ['first', 'last'],\n",
    "        'order_ts_diff': agg_func,\n",
    "        'order_ts_plus': ['first', 'last'],\n",
    "        'order_ts_plus_diff': agg_func,\n",
    "        'quantity_diff': agg_func,\n",
    "        'price_diff': agg_func,\n",
    "        'total_diff': agg_func,\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "        'cumsum_total_by_cust_id': agg_func,\n",
    "        'cumsum_quantity_by_cust_id': agg_func,\n",
    "        'cumsum_price_by_cust_id': agg_func,\n",
    "        'cumsum_total_by_prod_id': agg_func,\n",
    "        'cumsum_quantity_by_prod_id': agg_func,\n",
    "        'cumsum_price_by_prod_id': agg_func,\n",
    "        'cumsum_total_by_order_id': agg_func,\n",
    "        'cumsum_quantity_by_order_id': agg_func,\n",
    "        'cumsum_price_by_order_id': agg_func,\n",
    "        'order_id': ['nunique'],\n",
    "        'product_id': ['nunique'],\n",
    "        'month-mode': [mode_f],\n",
    "        'year_month-mode': [mode_f],\n",
    "    }\n",
    "    all_train_data = pd.DataFrame()\n",
    "\n",
    "    for i, tr_ym in enumerate(train_label['year_month'].unique()):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        train_agg = train.loc[train['order_date'] < tr_ym].groupby(['customer_id']).agg(agg_dict)\n",
    "\n",
    "        new_cols = []\n",
    "        for col in agg_dict.keys():\n",
    "            for stat in agg_dict[col]:\n",
    "                if type(stat) is str:\n",
    "                    new_cols.append(f'{col}-{stat}')\n",
    "                else:\n",
    "                    new_cols.append(f'{col}-mode')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        train_agg['year_month'] = tr_ym\n",
    "\n",
    "        all_train_data = all_train_data.append(train_agg)\n",
    "\n",
    "    all_train_data = train_label.merge(all_train_data, on=['customer_id', 'year_month'], how='left')\n",
    "    all_train_data['cycle_1224'] = train_1224.to_numpy()\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    data = pd.read_csv(\"/opt/ml/code/input/train.csv\", parse_dates=[\"order_date\"])\n",
    "    # # baseline feature engineering\n",
    "    # train, test, y, features = feature_engineering(data, '2011-12')\n",
    "    # trend\n",
    "    train_t, test_t = add_trend(data, year_month='2011-12')\n",
    "    # seasonality\n",
    "    train_s, test_s = add_seasonality(data, year_month='2011-12')\n",
    "    # train 데이터 병합\n",
    "    all_train_data = all_train_data.merge(train_t, on=['customer_id'], how='left')\n",
    "    all_train_data = all_train_data.merge(train_s, on=['customer_id'], how='left')\n",
    "    all_train_data = all_train_data.fillna(0)\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    features = all_train_data.drop(columns=['customer_id', 'label', 'year_month']).columns\n",
    "    print(features.shape)\n",
    "\n",
    "    import csv\n",
    "    with open(\"../output/feature.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for items in features.tolist():\n",
    "            print(items)\n",
    "            writer.writerow([items])\n",
    "\n",
    "    test_agg = test.groupby(['customer_id']).agg(agg_dict)\n",
    "    test_agg.columns = new_cols\n",
    "    test_agg['cycle_1224'] = test_1224\n",
    "\n",
    "    test_data = test_label.merge(test_agg, on=['customer_id'], how='left')\n",
    "\n",
    "    # test 데이터 병합 ===================================================================================\n",
    "    test_data = test_data.merge(test_t, on=['customer_id'], how='left')\n",
    "    test_data = test_data.merge(test_s, on=['customer_id'], how='left')\n",
    "    test_data = test_data.fillna(0)\n",
    "\n",
    "    # train, test 데이터 전처리\n",
    "    print(all_train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    x_tr, x_te = feature_preprocessing(all_train_data, test_data, features)\n",
    "    print('x_tr.shape', x_tr.shape, ', x_te.shape', x_te.shape)\n",
    "\n",
    "    return x_tr, x_te, all_train_data['label'], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed32deb7",
   "metadata": {
    "id": "c18bfc68"
   },
   "outputs": [],
   "source": [
    "TOTAL_THRES = 300\n",
    "\n",
    "'''\n",
    "    입력인자로 받는 year_month에 대해 고객 ID별로 총 구매액이\n",
    "    구매액 임계값을 넘는지 여부의 binary label을 생성하는 함수\n",
    "'''\n",
    "def generate_label(df, year_month, total_thres=TOTAL_THRES, print_log=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # year_month에 해당하는 label 데이터 생성\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # year_month 이전 월의 고객 ID 추출\n",
    "    cust = df[df['year_month']<year_month]['customer_id'].unique()\n",
    "    # year_month에 해당하는 데이터 선택\n",
    "    df = df[df['year_month']==year_month]\n",
    "    \n",
    "    # label 데이터프레임 생성\n",
    "    label = pd.DataFrame({'customer_id':cust})\n",
    "    label['year_month'] = year_month\n",
    "    \n",
    "    # year_month에 해당하는 고객 ID의 구매액의 합 계산\n",
    "    grped = df.groupby(['customer_id','year_month'], as_index=False)[['total']].sum()\n",
    "    \n",
    "    # label 데이터프레임과 merge하고 구매액 임계값을 넘었는지 여부로 label 생성\n",
    "    label = label.merge(grped, on=['customer_id','year_month'], how='left')\n",
    "    label['total'].fillna(0.0, inplace=True)\n",
    "    label['label'] = (label['total'] > total_thres).astype(int)\n",
    "\n",
    "    # 고객 ID로 정렬\n",
    "    label = label.sort_values('customer_id').reset_index(drop=True)\n",
    "    if print_log: print(f'{year_month} - final label shape: {label.shape}')\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74092589",
   "metadata": {
    "id": "634f74b8",
    "outputId": "154fb733-9f7d-4b30-84ec-dfb3f3d76715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780502, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>order_date</th>\n",
       "      <th>price</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>country</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489434</td>\n",
       "      <td>85048</td>\n",
       "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>11.4675</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>137.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323P</td>\n",
       "      <td>PINK CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>11.1375</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>133.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489434</td>\n",
       "      <td>79323W</td>\n",
       "      <td>WHITE CHERRY LIGHTS</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>11.1375</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>133.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489434</td>\n",
       "      <td>22041</td>\n",
       "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
       "      <td>48</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>3.4650</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>166.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489434</td>\n",
       "      <td>21232</td>\n",
       "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
       "      <td>24</td>\n",
       "      <td>2009-12-01 07:45:00</td>\n",
       "      <td>2.0625</td>\n",
       "      <td>13085</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>49.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_id product_id                          description  quantity  \\\n",
       "0   489434      85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
       "1   489434     79323P                   PINK CHERRY LIGHTS        12   \n",
       "2   489434     79323W                  WHITE CHERRY LIGHTS        12   \n",
       "3   489434      22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
       "4   489434      21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
       "\n",
       "           order_date    price  customer_id         country   total  \n",
       "0 2009-12-01 07:45:00  11.4675        13085  United Kingdom  137.61  \n",
       "1 2009-12-01 07:45:00  11.1375        13085  United Kingdom  133.65  \n",
       "2 2009-12-01 07:45:00  11.1375        13085  United Kingdom  133.65  \n",
       "3 2009-12-01 07:45:00   3.4650        13085  United Kingdom  166.32  \n",
       "4 2009-12-01 07:45:00   2.0625        13085  United Kingdom   49.50  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/train.csv\", parse_dates=[\"order_date\"])\n",
    "print(data.shape) # data shape 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28105085",
   "metadata": {
    "id": "b96ab6f0",
    "outputId": "a00d700c-4fcb-4656-9dfa-b6e62afe3e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380,)\n",
      "order_ts-first\n",
      "order_ts-last\n",
      "order_ts_diff-mean\n",
      "order_ts_diff-max\n",
      "order_ts_diff-min\n",
      "order_ts_diff-sum\n",
      "order_ts_diff-count\n",
      "order_ts_diff-std\n",
      "order_ts_diff-skew\n",
      "order_ts_plus-first\n",
      "order_ts_plus-last\n",
      "order_ts_plus_diff-mean\n",
      "order_ts_plus_diff-max\n",
      "order_ts_plus_diff-min\n",
      "order_ts_plus_diff-sum\n",
      "order_ts_plus_diff-count\n",
      "order_ts_plus_diff-std\n",
      "order_ts_plus_diff-skew\n",
      "quantity_diff-mean\n",
      "quantity_diff-max\n",
      "quantity_diff-min\n",
      "quantity_diff-sum\n",
      "quantity_diff-count\n",
      "quantity_diff-std\n",
      "quantity_diff-skew\n",
      "price_diff-mean\n",
      "price_diff-max\n",
      "price_diff-min\n",
      "price_diff-sum\n",
      "price_diff-count\n",
      "price_diff-std\n",
      "price_diff-skew\n",
      "total_diff-mean\n",
      "total_diff-max\n",
      "total_diff-min\n",
      "total_diff-sum\n",
      "total_diff-count\n",
      "total_diff-std\n",
      "total_diff-skew\n",
      "quantity-mean\n",
      "quantity-max\n",
      "quantity-min\n",
      "quantity-sum\n",
      "quantity-count\n",
      "quantity-std\n",
      "quantity-skew\n",
      "price-mean\n",
      "price-max\n",
      "price-min\n",
      "price-sum\n",
      "price-count\n",
      "price-std\n",
      "price-skew\n",
      "total-mean\n",
      "total-max\n",
      "total-min\n",
      "total-sum\n",
      "total-count\n",
      "total-std\n",
      "total-skew\n",
      "cumsum_total_by_cust_id-mean\n",
      "cumsum_total_by_cust_id-max\n",
      "cumsum_total_by_cust_id-min\n",
      "cumsum_total_by_cust_id-sum\n",
      "cumsum_total_by_cust_id-count\n",
      "cumsum_total_by_cust_id-std\n",
      "cumsum_total_by_cust_id-skew\n",
      "cumsum_quantity_by_cust_id-mean\n",
      "cumsum_quantity_by_cust_id-max\n",
      "cumsum_quantity_by_cust_id-min\n",
      "cumsum_quantity_by_cust_id-sum\n",
      "cumsum_quantity_by_cust_id-count\n",
      "cumsum_quantity_by_cust_id-std\n",
      "cumsum_quantity_by_cust_id-skew\n",
      "cumsum_price_by_cust_id-mean\n",
      "cumsum_price_by_cust_id-max\n",
      "cumsum_price_by_cust_id-min\n",
      "cumsum_price_by_cust_id-sum\n",
      "cumsum_price_by_cust_id-count\n",
      "cumsum_price_by_cust_id-std\n",
      "cumsum_price_by_cust_id-skew\n",
      "cumsum_total_by_prod_id-mean\n",
      "cumsum_total_by_prod_id-max\n",
      "cumsum_total_by_prod_id-min\n",
      "cumsum_total_by_prod_id-sum\n",
      "cumsum_total_by_prod_id-count\n",
      "cumsum_total_by_prod_id-std\n",
      "cumsum_total_by_prod_id-skew\n",
      "cumsum_quantity_by_prod_id-mean\n",
      "cumsum_quantity_by_prod_id-max\n",
      "cumsum_quantity_by_prod_id-min\n",
      "cumsum_quantity_by_prod_id-sum\n",
      "cumsum_quantity_by_prod_id-count\n",
      "cumsum_quantity_by_prod_id-std\n",
      "cumsum_quantity_by_prod_id-skew\n",
      "cumsum_price_by_prod_id-mean\n",
      "cumsum_price_by_prod_id-max\n",
      "cumsum_price_by_prod_id-min\n",
      "cumsum_price_by_prod_id-sum\n",
      "cumsum_price_by_prod_id-count\n",
      "cumsum_price_by_prod_id-std\n",
      "cumsum_price_by_prod_id-skew\n",
      "cumsum_total_by_order_id-mean\n",
      "cumsum_total_by_order_id-max\n",
      "cumsum_total_by_order_id-min\n",
      "cumsum_total_by_order_id-sum\n",
      "cumsum_total_by_order_id-count\n",
      "cumsum_total_by_order_id-std\n",
      "cumsum_total_by_order_id-skew\n",
      "cumsum_quantity_by_order_id-mean\n",
      "cumsum_quantity_by_order_id-max\n",
      "cumsum_quantity_by_order_id-min\n",
      "cumsum_quantity_by_order_id-sum\n",
      "cumsum_quantity_by_order_id-count\n",
      "cumsum_quantity_by_order_id-std\n",
      "cumsum_quantity_by_order_id-skew\n",
      "cumsum_price_by_order_id-mean\n",
      "cumsum_price_by_order_id-max\n",
      "cumsum_price_by_order_id-min\n",
      "cumsum_price_by_order_id-sum\n",
      "cumsum_price_by_order_id-count\n",
      "cumsum_price_by_order_id-std\n",
      "cumsum_price_by_order_id-skew\n",
      "order_id-nunique\n",
      "product_id-nunique\n",
      "month-mode-mode\n",
      "year_month-mode-mode\n",
      "cycle_1224\n",
      "quantity-max-0\n",
      "quantity-min-0\n",
      "quantity-sum-0\n",
      "quantity-mean-0\n",
      "quantity-count-0\n",
      "quantity-std-0\n",
      "quantity-skew-0\n",
      "price-max-0\n",
      "price-min-0\n",
      "price-sum-0\n",
      "price-mean-0\n",
      "price-count-0\n",
      "price-std-0\n",
      "price-skew-0\n",
      "total-max-0\n",
      "total-min-0\n",
      "total-sum-0\n",
      "total-mean-0\n",
      "total-count-0\n",
      "total-std-0\n",
      "total-skew-0\n",
      "quantity-max-1\n",
      "quantity-min-1\n",
      "quantity-sum-1\n",
      "quantity-mean-1\n",
      "quantity-count-1\n",
      "quantity-std-1\n",
      "quantity-skew-1\n",
      "price-max-1\n",
      "price-min-1\n",
      "price-sum-1\n",
      "price-mean-1\n",
      "price-count-1\n",
      "price-std-1\n",
      "price-skew-1\n",
      "total-max-1\n",
      "total-min-1\n",
      "total-sum-1\n",
      "total-mean-1\n",
      "total-count-1\n",
      "total-std-1\n",
      "total-skew-1\n",
      "quantity-max-2\n",
      "quantity-min-2\n",
      "quantity-sum-2\n",
      "quantity-mean-2\n",
      "quantity-count-2\n",
      "quantity-std-2\n",
      "quantity-skew-2\n",
      "price-max-2\n",
      "price-min-2\n",
      "price-sum-2\n",
      "price-mean-2\n",
      "price-count-2\n",
      "price-std-2\n",
      "price-skew-2\n",
      "total-max-2\n",
      "total-min-2\n",
      "total-sum-2\n",
      "total-mean-2\n",
      "total-count-2\n",
      "total-std-2\n",
      "total-skew-2\n",
      "quantity-max-3\n",
      "quantity-min-3\n",
      "quantity-sum-3\n",
      "quantity-mean-3\n",
      "quantity-count-3\n",
      "quantity-std-3\n",
      "quantity-skew-3\n",
      "price-max-3\n",
      "price-min-3\n",
      "price-sum-3\n",
      "price-mean-3\n",
      "price-count-3\n",
      "price-std-3\n",
      "price-skew-3\n",
      "total-max-3\n",
      "total-min-3\n",
      "total-sum-3\n",
      "total-mean-3\n",
      "total-count-3\n",
      "total-std-3\n",
      "total-skew-3\n",
      "quantity-max-4\n",
      "quantity-min-4\n",
      "quantity-sum-4\n",
      "quantity-mean-4\n",
      "quantity-count-4\n",
      "quantity-std-4\n",
      "quantity-skew-4\n",
      "price-max-4\n",
      "price-min-4\n",
      "price-sum-4\n",
      "price-mean-4\n",
      "price-count-4\n",
      "price-std-4\n",
      "price-skew-4\n",
      "total-max-4\n",
      "total-min-4\n",
      "total-sum-4\n",
      "total-mean-4\n",
      "total-count-4\n",
      "total-std-4\n",
      "total-skew-4\n",
      "quantity-max-5\n",
      "quantity-min-5\n",
      "quantity-sum-5\n",
      "quantity-mean-5\n",
      "quantity-count-5\n",
      "quantity-std-5\n",
      "quantity-skew-5\n",
      "price-max-5\n",
      "price-min-5\n",
      "price-sum-5\n",
      "price-mean-5\n",
      "price-count-5\n",
      "price-std-5\n",
      "price-skew-5\n",
      "total-max-5\n",
      "total-min-5\n",
      "total-sum-5\n",
      "total-mean-5\n",
      "total-count-5\n",
      "total-std-5\n",
      "total-skew-5\n",
      "quantity-max-6\n",
      "quantity-min-6\n",
      "quantity-sum-6\n",
      "quantity-mean-6\n",
      "quantity-count-6\n",
      "quantity-std-6\n",
      "quantity-skew-6\n",
      "price-max-6\n",
      "price-min-6\n",
      "price-sum-6\n",
      "price-mean-6\n",
      "price-count-6\n",
      "price-std-6\n",
      "price-skew-6\n",
      "total-max-6\n",
      "total-min-6\n",
      "total-sum-6\n",
      "total-mean-6\n",
      "total-count-6\n",
      "total-std-6\n",
      "total-skew-6\n",
      "quantity-max-7\n",
      "quantity-min-7\n",
      "quantity-sum-7\n",
      "quantity-mean-7\n",
      "quantity-count-7\n",
      "quantity-std-7\n",
      "quantity-skew-7\n",
      "price-max-7\n",
      "price-min-7\n",
      "price-sum-7\n",
      "price-mean-7\n",
      "price-count-7\n",
      "price-std-7\n",
      "price-skew-7\n",
      "total-max-7\n",
      "total-min-7\n",
      "total-sum-7\n",
      "total-mean-7\n",
      "total-count-7\n",
      "total-std-7\n",
      "total-skew-7\n",
      "quantity-max-season0\n",
      "quantity-min-season0\n",
      "quantity-sum-season0\n",
      "quantity-mean-season0\n",
      "quantity-count-season0\n",
      "quantity-std-season0\n",
      "quantity-skew-season0\n",
      "price-max-season0\n",
      "price-min-season0\n",
      "price-sum-season0\n",
      "price-mean-season0\n",
      "price-count-season0\n",
      "price-std-season0\n",
      "price-skew-season0\n",
      "total-max-season0\n",
      "total-min-season0\n",
      "total-sum-season0\n",
      "total-mean-season0\n",
      "total-count-season0\n",
      "total-std-season0\n",
      "total-skew-season0\n",
      "quantity-max-season1\n",
      "quantity-min-season1\n",
      "quantity-sum-season1\n",
      "quantity-mean-season1\n",
      "quantity-count-season1\n",
      "quantity-std-season1\n",
      "quantity-skew-season1\n",
      "price-max-season1\n",
      "price-min-season1\n",
      "price-sum-season1\n",
      "price-mean-season1\n",
      "price-count-season1\n",
      "price-std-season1\n",
      "price-skew-season1\n",
      "total-max-season1\n",
      "total-min-season1\n",
      "total-sum-season1\n",
      "total-mean-season1\n",
      "total-count-season1\n",
      "total-std-season1\n",
      "total-skew-season1\n",
      "quantity-max-season2\n",
      "quantity-min-season2\n",
      "quantity-sum-season2\n",
      "quantity-mean-season2\n",
      "quantity-count-season2\n",
      "quantity-std-season2\n",
      "quantity-skew-season2\n",
      "price-max-season2\n",
      "price-min-season2\n",
      "price-sum-season2\n",
      "price-mean-season2\n",
      "price-count-season2\n",
      "price-std-season2\n",
      "price-skew-season2\n",
      "total-max-season2\n",
      "total-min-season2\n",
      "total-sum-season2\n",
      "total-mean-season2\n",
      "total-count-season2\n",
      "total-std-season2\n",
      "total-skew-season2\n",
      "quantity-max-season3\n",
      "quantity-min-season3\n",
      "quantity-sum-season3\n",
      "quantity-mean-season3\n",
      "quantity-count-season3\n",
      "quantity-std-season3\n",
      "quantity-skew-season3\n",
      "price-max-season3\n",
      "price-min-season3\n",
      "price-sum-season3\n",
      "price-mean-season3\n",
      "price-count-season3\n",
      "price-std-season3\n",
      "price-skew-season3\n",
      "total-max-season3\n",
      "total-min-season3\n",
      "total-sum-season3\n",
      "total-mean-season3\n",
      "total-count-season3\n",
      "total-std-season3\n",
      "total-skew-season3\n",
      "(5722, 383)\n",
      "(5914, 383)\n",
      "categorical feature: ['year_month-mode-mode']\n",
      "x_tr.shape (5722, 383) , x_te.shape (5914, 383)\n"
     ]
    }
   ],
   "source": [
    "train, test, y, features = feature_engineering2(data, '2011-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0fa503c",
   "metadata": {
    "id": "66df3f63"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# quan = QuantileTransformer(n_quantiles=2000, output_distribution='normal',random_state=42)\n",
    "quan = QuantileTransformer(n_quantiles=2000, random_state=42) # 1000에서 제일 좋았음\n",
    "X_quan = quan.fit_transform(train[features])\n",
    "Y_quan = quan.fit_transform(test[features])\n",
    "x_quan = pd.DataFrame(X_quan, columns=features)\n",
    "y_quan = pd.DataFrame(Y_quan, columns=features)\n",
    "train[features] = x_quan[features]\n",
    "test[features] = y_quan[features]\n",
    "\n",
    "train['customer_id'] = train['customer_id'].astype('category')\n",
    "test['customer_id'] = test['customer_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "645d1467",
   "metadata": {
    "id": "147a0068"
   },
   "outputs": [],
   "source": [
    "# year_month 이전 월 계산\n",
    "d = datetime.datetime.strptime('2011-12', \"%Y-%m\")\n",
    "prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "prev_ym = prev_ym.strftime('%Y-%m')\n",
    "\n",
    "des_col = data.loc[data['order_date'] < prev_ym].groupby(['customer_id'])['description'].apply(lambda x: ', '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54453ca6",
   "metadata": {
    "id": "4c5b70ec"
   },
   "outputs": [],
   "source": [
    "trn_df = train.merge(des_col, on=['customer_id'], how='left')\n",
    "tst_df = test.merge(des_col, on=['customer_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6caaf186",
   "metadata": {
    "id": "096a7b20"
   },
   "outputs": [],
   "source": [
    "trn_df.to_csv('../output/train_des.csv', index=False)\n",
    "tst_df.to_csv('../output/test_des.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bea48fa",
   "metadata": {
    "id": "43971d4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210422_084348/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210422_084348/\"\n",
      "AutoGluon Version:  0.1.0\n",
      "Train Data Rows:    5722\n",
      "Train Data Columns: 382\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    44743.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 18.0 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['year_month', 'order_ts_plus_diff-min']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['customer_id']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['customer_id']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 379 | ['order_ts-first', 'order_ts-last', 'order_ts_diff-mean', 'order_ts_diff-max', 'order_ts_diff-min', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 379 | ['order_ts-first', 'order_ts-last', 'order_ts_diff-mean', 'order_ts_diff-max', 'order_ts_diff-min', ...]\n",
      "\t1.7s = Fit runtime\n",
      "\t379 features in original data used to generate 379 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 17.35 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5149, Val Rows: 573\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8005\t = Validation roc_auc score\n",
      "\t7.26s\t = Training runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8245\t = Validation roc_auc score\n",
      "\t9.78s\t = Training runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8203\t = Validation roc_auc score\n",
      "\t3.18s\t = Training runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8304\t = Validation roc_auc score\n",
      "\t3.28s\t = Training runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.749\t = Validation roc_auc score\n",
      "\t0.06s\t = Training runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7475\t = Validation roc_auc score\n",
      "\t0.03s\t = Training runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.8222\t = Validation roc_auc score\n",
      "\t32.31s\t = Training runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.8429\t = Validation roc_auc score\n",
      "\t35.91s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.8317\t = Validation roc_auc score\n",
      "\t37.76s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8295\t = Validation roc_auc score\n",
      "\t54.31s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ...\n",
      "\t0.8438\t = Validation roc_auc score\n",
      "\t13.93s\t = Training runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8361\t = Validation roc_auc score\n",
      "\t60.12s\t = Training runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.8251\t = Validation roc_auc score\n",
      "\t66.87s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8524\t = Validation roc_auc score\n",
      "\t1.94s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 335.87s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210422_084348/\")\n",
      "Loaded data from: ../output/test_des.csv | Columns = 384 / 384 | Rows = 5914 -> 5914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "predictor = TabularPredictor(label='label', eval_metric='roc_auc').fit(train)\n",
    "\n",
    "\n",
    "pred = predictor.predict_proba('../output/test_des.csv')\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['probability'] = pred[1]\n",
    "sub.to_csv('../output/autoGluon.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899baad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "concat_describ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
