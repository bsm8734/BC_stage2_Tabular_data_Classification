{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f06d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'boostcamp_source'...\n",
      "remote: Enumerating objects: 76, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
      "remote: Total 76 (delta 25), reused 59 (delta 15), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (76/76), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DShomin/boostcamp_source.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778d3e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in /opt/conda/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.24.1)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.5.4)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.46.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32d4399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys, gc, warnings, random\n",
    "\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "data_dir = '../input' # os.environ['SM_CHANNEL_TRAIN']\n",
    "model_dir = '../model' # os.environ['SM_MODEL_DIR']\n",
    "output_dir = '../output' # os.environ['SM_OUTPUT_DATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5621ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855dfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_THRES = 300\n",
    "\n",
    "def generate_label(df, year_month, total_thres=TOTAL_THRES, print_log=False):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # year_month에 해당하는 label 데이터 생성\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # year_month 이전 월의 고객 ID 추출\n",
    "    cust = df[df['year_month']<year_month]['customer_id'].unique()\n",
    "    # year_month에 해당하는 데이터 선택\n",
    "    df = df[df['year_month']==year_month]\n",
    "    \n",
    "    # label 데이터프레임 생성\n",
    "    label = pd.DataFrame({'customer_id':cust})\n",
    "    label['year_month'] = year_month\n",
    "    \n",
    "    # year_month에 해당하는 고객 ID의 구매액의 합 계산\n",
    "    grped = df.groupby(['customer_id','year_month'], as_index=False)[['total']].sum()\n",
    "    \n",
    "    # label 데이터프레임과 merge하고 구매액 임계값을 넘었는지 여부로 label 생성\n",
    "    label = label.merge(grped, on=['customer_id','year_month'], how='left')\n",
    "    label['total'].fillna(0.0, inplace=True)\n",
    "    label['label'] = (label['total'] > total_thres).astype(int)\n",
    "\n",
    "    # 고객 ID로 정렬\n",
    "    label = label.sort_values('customer_id').reset_index(drop=True)\n",
    "    if print_log: print(f'{year_month} - final label shape: {label.shape}')\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dee854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(label, pred, prob_thres=0.5):\n",
    "    print('Precision: {:.5f}'.format(precision_score(label, pred>prob_thres)))\n",
    "    print('Recall: {:.5f}'.format(recall_score(label, pred>prob_thres)))\n",
    "    print('F1 Score: {:.5f}'.format(f1_score(label, pred>prob_thres)))\n",
    "    print('ROC AUC Score: {:.5f}'.format(roc_auc_score(label, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee35a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_preprocessing(train, test, features, do_imputing=True):\n",
    "    x_tr = train.copy()\n",
    "    x_te = test.copy()\n",
    "    \n",
    "    # 범주형 피처 이름을 저장할 변수\n",
    "    cate_cols = []\n",
    "\n",
    "    # 레이블 인코딩\n",
    "    for f in features:\n",
    "        if x_tr[f].dtype.name == 'object': # 데이터 타입이 object(str)이면 레이블 인코딩\n",
    "            cate_cols.append(f)\n",
    "            le = LabelEncoder()\n",
    "            # train + test 데이터를 합쳐서 레이블 인코딩 함수에 fit\n",
    "            le.fit(list(x_tr[f].values) + list(x_te[f].values))\n",
    "            \n",
    "            # train 데이터 레이블 인코딩 변환 수행\n",
    "            x_tr[f] = le.transform(list(x_tr[f].values))\n",
    "            \n",
    "            # test 데이터 레이블 인코딩 변환 수행\n",
    "            x_te[f] = le.transform(list(x_te[f].values))\n",
    "\n",
    "    print('categorical feature:', cate_cols)\n",
    "\n",
    "    if do_imputing:\n",
    "        # 중위값으로 결측치 채우기\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        x_tr[features] = imputer.fit_transform(x_tr[features])\n",
    "        x_te[features] = imputer.transform(x_te[features])\n",
    "    \n",
    "    return x_tr, x_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93fa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_time_series_data(df, Input, year_month, stand):\n",
    "    # 기준을 잡습니다. 기준은 여기서 %Y-%m 입니다.\n",
    "    standard = ['customer_id'] + [stand]\n",
    "    data = Input.copy()\n",
    "    df = df.copy()\n",
    "\n",
    "    data[stand] = pd.to_datetime(df['order_date']).dt.strftime(stand)\n",
    "    data.order_date = pd.to_datetime(data['order_date'])\n",
    "\n",
    "    # 월단위의 틀을 만들어주고, 기준으로 aggregation을 해준 다음에 merge를 해줄 것입니다\n",
    "    times = pd.date_range('2009-12-01', periods=(data.order_date.max() - data.order_date.min()).days + 1, freq='1d')\n",
    "    customerid_frame = np.repeat(data.customer_id.unique(), len(times))\n",
    "    date_frame = np.tile(times, len(data.customer_id.unique()))\n",
    "\n",
    "    frame = pd.DataFrame({'customer_id': customerid_frame, 'order_date': date_frame})\n",
    "    frame[stand] = pd.to_datetime(frame.order_date).dt.strftime(stand)\n",
    "\n",
    "    # group by\n",
    "    data_group = data.groupby(standard).sum().reset_index()\n",
    "    frame_group = frame.groupby(standard).count().reset_index().drop(['order_date'], axis=1)\n",
    "\n",
    "    # merge\n",
    "    merge = pd.merge(frame_group, data_group, on=standard, how='left').fillna(0)\n",
    "    merge = merge.rename(columns={stand: 'standard'})\n",
    "\n",
    "    merge_test = merge[merge['standard'] == year_month].drop(columns=['standard', 'quantity', 'price']) #.drop(merge.columns.tolist() - ['customer_id', 'total'])\n",
    "    return merge_test\n",
    "\n",
    "def add_trend(df, year_month):\n",
    "    df = df.copy()\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    # train과 test 데이터 생성\n",
    "    train = df[df['order_date'] < prev_ym]  # 2009-12부터 2011-10 데이터 추출\n",
    "    test = df[df['order_date'] < year_month]  # 2009-12부터 2011-11 데이터 추출\n",
    "    train_window_ym = []\n",
    "    test_window_ym = []\n",
    "    for month_back in [1, 2, 3, 5, 7, 12, 20, 23]:  # 1개월, 2개월, ... 20개월, 23개월 전 year_month 파악\n",
    "        train_window_ym.append((prev_ym - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'))\n",
    "        test_window_ym.append((d - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'))\n",
    "    # aggregation 함수 선언\n",
    "    agg_func = ['max', 'min', 'sum', 'mean', 'count', 'std', 'skew']\n",
    "    # group by aggregation with Dictionary\n",
    "    agg_dict = {\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "    }\n",
    "    # general statistics for train data with time series trend\n",
    "    for i, tr_ym in enumerate(train_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        train_agg = train.loc[train['year_month'] >= tr_ym].groupby(['customer_id']).agg(\n",
    "            agg_dict)  # 해당 year_month 이후부터 모든 데이터에 대한 aggregation을 실시\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in train_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-{i}')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            train_data = train_agg\n",
    "        else:\n",
    "            train_data = train_data.merge(train_agg, on=['customer_id'], how='right')\n",
    "    # general statistics for test data with time series trend\n",
    "    for i, tr_ym in enumerate(test_window_ym):\n",
    "        # group by aggretation 함수로 test 데이터 피처 생성\n",
    "        test_agg = test.loc[test['year_month'] >= tr_ym].groupby(['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in test_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-{i}')\n",
    "        test_agg.columns = new_cols\n",
    "        test_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            test_data = test_agg\n",
    "        else:\n",
    "            test_data = test_data.merge(test_agg, on=['customer_id'], how='right')\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def add_seasonality(df, year_month):\n",
    "    df = df.copy()\n",
    "    df['year_month'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    # train과 test 데이터 생성\n",
    "    train = df[df['order_date'] < prev_ym]  # 2009-12부터 2011-10 데이터 추출\n",
    "    test = df[df['order_date'] < year_month]  # 2009-12부터 2011-11 데이터 추출\n",
    "    train_window_ym = []\n",
    "    test_window_ym = []\n",
    "    for month_back in [1, 6, 12, 18]:  # 각 주기성을 파악하고 싶은 구간을 생성\n",
    "        train_window_ym.append(\n",
    "            (\n",
    "                (prev_ym - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'),\n",
    "                (prev_ym - dateutil.relativedelta.relativedelta(months=month_back + 2)).strftime('%Y-%m')\n",
    "            # 1~3, 6~8, 12~14, 18~20 Pair를 만들어준다\n",
    "            )\n",
    "        )\n",
    "        test_window_ym.append(\n",
    "            (\n",
    "                (d - dateutil.relativedelta.relativedelta(months=month_back)).strftime('%Y-%m'),\n",
    "                (d - dateutil.relativedelta.relativedelta(months=month_back + 2)).strftime('%Y-%m')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # aggregation 함수 선언\n",
    "    agg_func = ['max', 'min', 'sum', 'mean', 'count', 'std', 'skew']\n",
    "    # group by aggregation with Dictionary\n",
    "    agg_dict = {\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "    }\n",
    "    # seasonality for train data with time series\n",
    "    for i, (tr_ym, tr_ym_3) in enumerate(train_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        # 구간 사이에 존재하는 월들에 대해서 aggregation을 진행\n",
    "        train_agg = train.loc[(train['year_month'] >= tr_ym_3) & (train['year_month'] <= tr_ym)].groupby(\n",
    "            ['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in train_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-season{i}')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            train_data = train_agg\n",
    "        else:\n",
    "            train_data = train_data.merge(train_agg, on=['customer_id'], how='right')\n",
    "    # seasonality for test data with time series\n",
    "    for i, (tr_ym, tr_ym_3) in enumerate(test_window_ym):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        test_agg = test.loc[(test['year_month'] >= tr_ym_3) & (test['year_month'] <= tr_ym)].groupby(\n",
    "            ['customer_id']).agg(agg_dict)\n",
    "        # 멀티 레벨 컬럼을 사용하기 쉽게 1 레벨 컬럼명으로 변경\n",
    "        new_cols = []\n",
    "        for level1, level2 in test_agg.columns:\n",
    "            new_cols.append(f'{level1}-{level2}-season{i}')\n",
    "        test_agg.columns = new_cols\n",
    "        test_agg.reset_index(inplace=True)\n",
    "\n",
    "        if i == 0:\n",
    "            test_data = test_agg\n",
    "        else:\n",
    "            test_data = test_data.merge(test_agg, on=['customer_id'], how='right')\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering2(df, year_month):\n",
    "    df = df.copy()\n",
    "\n",
    "    # customer_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_cust_id'] = df.groupby(['customer_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_cust_id'] = df.groupby(['customer_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_cust_id'] = df.groupby(['customer_id'])['price'].cumsum()\n",
    "\n",
    "    # product_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_prod_id'] = df.groupby(['product_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_prod_id'] = df.groupby(['product_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_prod_id'] = df.groupby(['product_id'])['price'].cumsum()\n",
    "\n",
    "    # order_id 기준으로 pandas group by 후 total, quantity, price 누적합 계산\n",
    "    df['cumsum_total_by_order_id'] = df.groupby(['order_id'])['total'].cumsum()\n",
    "    df['cumsum_quantity_by_order_id'] = df.groupby(['order_id'])['quantity'].cumsum()\n",
    "    df['cumsum_price_by_order_id'] = df.groupby(['order_id'])['price'].cumsum()\n",
    "\n",
    "    # oredr_ts\n",
    "    df['order_ts'] = df['order_date'].astype(np.int64)//1e9\n",
    "    df['order_ts_diff'] = df.groupby(['customer_id'])['order_ts'].diff()\n",
    "    df['quantity_diff'] = df.groupby(['customer_id'])['quantity'].diff()\n",
    "    df['price_diff'] = df.groupby(['customer_id'])['price'].diff()\n",
    "    df['total_diff'] = df.groupby(['customer_id'])['total'].diff()\n",
    "\n",
    "    # mode\n",
    "    df['month-mode'] = df['order_date'].dt.month\n",
    "    df['year_month-mode'] = df['order_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "    # oredr_ts_plus ===\n",
    "    df['order_ts_plus'] = df[df['total'] > 0]['order_date'].astype(np.int64) // 1e9\n",
    "    df['order_ts_plus_diff'] = df[df['total'] > 0].groupby(['customer_id'])['order_ts'].diff()\n",
    "    df['order_ts_plus'] = df['order_ts_plus'].fillna(0)\n",
    "    df['order_ts_plus_diff'] = df['order_ts_plus_diff'].fillna(0)\n",
    "    # df[~(df.order_id.str.contains('C'))].groupby(['customer_id'])['order_date'].last().astype(np.int64) // 1e9\n",
    "\n",
    "    # ================================================================================================\n",
    "    # year_month 이전 월 계산\n",
    "    d = datetime.datetime.strptime(year_month, \"%Y-%m\")\n",
    "    prev_ym = d - dateutil.relativedelta.relativedelta(months=1)\n",
    "    prev_ym = prev_ym.strftime('%Y-%m')\n",
    "\n",
    "    # train, test 데이터 선택\n",
    "    train = df[df['order_date'] < prev_ym]\n",
    "    test = df[df['order_date'] < year_month]\n",
    "\n",
    "    # train, test 레이블 데이터 생성\n",
    "    train_label = generate_label(df, prev_ym)[['customer_id', 'year_month', 'label']]\n",
    "    test_label = generate_label(df, year_month)[['customer_id', 'year_month', 'label']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 연월 피처 생성\n",
    "    target = datetime.datetime.strptime('2011-11', \"%Y-%m\")  # 타겟 연월\n",
    "    prev = target - dateutil.relativedelta.relativedelta(years=1)  # 전년 연월\n",
    "    prev = prev.strftime('%Y-%m')  # 문자열로 변환\n",
    "    groupby = train.groupby(['customer_id', 'year_month-mode'])['total'].sum()  # 고객별, 월별 total 합\n",
    "    groupby = groupby.unstack()  # 월별을 컬럼으로 변환\n",
    "    prev_pprev_total = groupby.loc[:, [prev]]  # 전년, 전전년 데이터만 추출\n",
    "    prev_pprev_total = prev_pprev_total.fillna(0)\n",
    "\n",
    "    train_1224 = (prev_pprev_total['2010-11']) / 2\n",
    "\n",
    "\n",
    "    target = datetime.datetime.strptime('2011-12', \"%Y-%m\")  # 타겟 연월\n",
    "    prev = target - dateutil.relativedelta.relativedelta(years=1)  # 전년 연월\n",
    "    pprev = prev - dateutil.relativedelta.relativedelta(years=1)  # 전전년 연월\n",
    "    prev, pprev = prev.strftime('%Y-%m'), pprev.strftime('%Y-%m')  # 문자열로 변환\n",
    "    groupby = test.groupby(['customer_id', 'year_month-mode'])['total'].sum()  # 고객별, 월별 total 합\n",
    "    groupby = groupby.unstack()  # 월별을 컬럼으로 변환\n",
    "    prev_pprev_total = groupby.loc[:, [prev, pprev]]  # 전년, 전전년 데이터만 추출\n",
    "    prev_pprev_total = prev_pprev_total.fillna(0)\n",
    "\n",
    "    test_1224 = (prev_pprev_total['2010-12'] + prev_pprev_total['2009-12']) / 2\n",
    "\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    # lambda 식\n",
    "    mode_f = lambda x: x.value_counts().index[0]\n",
    "\n",
    "    # group by aggregation 함수 선언\n",
    "    agg_func = ['mean', 'max', 'min', 'sum', 'count', 'std', 'skew']\n",
    "    # agg_func = ['mean', 'max'] # , 'min', 'sum', 'count', 'std', 'skew']\n",
    "    agg_dict = {\n",
    "        'order_ts': ['first', 'last'],\n",
    "        'order_ts_diff': agg_func,\n",
    "        'order_ts_plus': ['first', 'last'],\n",
    "        'order_ts_plus_diff': agg_func,\n",
    "        'quantity_diff': agg_func,\n",
    "        'price_diff': agg_func,\n",
    "        'total_diff': agg_func,\n",
    "        'quantity': agg_func,\n",
    "        'price': agg_func,\n",
    "        'total': agg_func,\n",
    "        'cumsum_total_by_cust_id': agg_func,\n",
    "        'cumsum_quantity_by_cust_id': agg_func,\n",
    "        'cumsum_price_by_cust_id': agg_func,\n",
    "        'cumsum_total_by_prod_id': agg_func,\n",
    "        'cumsum_quantity_by_prod_id': agg_func,\n",
    "        'cumsum_price_by_prod_id': agg_func,\n",
    "        'cumsum_total_by_order_id': agg_func,\n",
    "        'cumsum_quantity_by_order_id': agg_func,\n",
    "        'cumsum_price_by_order_id': agg_func,\n",
    "        'order_id': ['nunique'],\n",
    "        'product_id': ['nunique'],\n",
    "        'month-mode': [mode_f],\n",
    "        'year_month-mode': [mode_f],\n",
    "    }\n",
    "    all_train_data = pd.DataFrame()\n",
    "\n",
    "    for i, tr_ym in enumerate(train_label['year_month'].unique()):\n",
    "        # group by aggretation 함수로 train 데이터 피처 생성\n",
    "        train_agg = train.loc[train['order_date'] < tr_ym].groupby(['customer_id']).agg(agg_dict)\n",
    "\n",
    "        new_cols = []\n",
    "        for col in agg_dict.keys():\n",
    "            for stat in agg_dict[col]:\n",
    "                if type(stat) is str:\n",
    "                    new_cols.append(f'{col}-{stat}')\n",
    "                else:\n",
    "                    new_cols.append(f'{col}-mode')\n",
    "        train_agg.columns = new_cols\n",
    "        train_agg.reset_index(inplace=True)\n",
    "\n",
    "        train_agg['year_month'] = tr_ym\n",
    "\n",
    "        all_train_data = all_train_data.append(train_agg)\n",
    "\n",
    "    all_train_data = train_label.merge(all_train_data, on=['customer_id', 'year_month'], how='left')\n",
    "    all_train_data['cycle_1224'] = train_1224.to_numpy()\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    data = pd.read_csv(\"/opt/ml/code/input/train.csv\", parse_dates=[\"order_date\"])\n",
    "    # # baseline feature engineering\n",
    "    # train, test, y, features = feature_engineering(data, '2011-12')\n",
    "    # trend\n",
    "    train_t, test_t = add_trend(data, year_month='2011-12')\n",
    "    # seasonality\n",
    "    train_s, test_s = add_seasonality(data, year_month='2011-12')\n",
    "    # train 데이터 병합\n",
    "    all_train_data = all_train_data.merge(train_t, on=['customer_id'], how='left')\n",
    "    all_train_data = all_train_data.merge(train_s, on=['customer_id'], how='left')\n",
    "    all_train_data = all_train_data.fillna(0)\n",
    "\n",
    "    # ================================================================================================\n",
    "\n",
    "    features = all_train_data.drop(columns=['customer_id', 'label', 'year_month']).columns\n",
    "    print(features.shape)\n",
    "\n",
    "    import csv\n",
    "    with open(\"../output/feature.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for items in features.tolist():\n",
    "            print(items)\n",
    "            writer.writerow([items])\n",
    "\n",
    "    test_agg = test.groupby(['customer_id']).agg(agg_dict)\n",
    "    test_agg.columns = new_cols\n",
    "    test_agg['cycle_1224'] = test_1224\n",
    "\n",
    "    test_data = test_label.merge(test_agg, on=['customer_id'], how='left')\n",
    "\n",
    "    # test 데이터 병합 ===================================================================================\n",
    "    test_data = test_data.merge(test_t, on=['customer_id'], how='left')\n",
    "    test_data = test_data.merge(test_s, on=['customer_id'], how='left')\n",
    "    test_data = test_data.fillna(0)\n",
    "\n",
    "    # train, test 데이터 전처리\n",
    "    print(all_train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    x_tr, x_te = feature_preprocessing(all_train_data, test_data, features)\n",
    "    print('x_tr.shape', x_tr.shape, ', x_te.shape', x_te.shape)\n",
    "\n",
    "    return x_tr, x_te, all_train_data['label'], features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ef5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lgb_oof_prediction(train, y, test, features, categorical_features='auto', model_params=None, folds=10):\n",
    "    x_train = train[features]\n",
    "    x_test = test[features]\n",
    "    \n",
    "    # 테스트 데이터 예측값을 저장할 변수\n",
    "    test_preds = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Out Of Fold Validation 예측 데이터를 저장할 변수\n",
    "    y_oof = np.zeros(x_train.shape[0])\n",
    "    \n",
    "    # 폴드별 평균 Validation 스코어를 저장할 변수\n",
    "    score = 0\n",
    "    \n",
    "    # 피처 중요도를 저장할 데이터 프레임 선언\n",
    "    fi = pd.DataFrame()\n",
    "    fi['feature'] = features\n",
    "    \n",
    "    # Stratified K Fold 선언\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(x_train, y)):\n",
    "        # train index, validation index로 train 데이터를 나눔\n",
    "        x_tr, x_val = x_train.loc[tr_idx, features], x_train.loc[val_idx, features]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "        \n",
    "        print(f'fold: {fold+1}, x_tr.shape: {x_tr.shape}, x_val.shape: {x_val.shape}')\n",
    "\n",
    "        # LightGBM 데이터셋 선언\n",
    "        dtrain = lgb.Dataset(x_tr, label=y_tr)\n",
    "        dvalid = lgb.Dataset(x_val, label=y_val)\n",
    "        \n",
    "        # LightGBM 모델 훈련\n",
    "        clf = lgb.train(\n",
    "            model_params,\n",
    "            dtrain,\n",
    "            valid_sets=[dtrain, dvalid], # Validation 성능을 측정할 수 있도록 설정\n",
    "            categorical_feature=categorical_features,\n",
    "            verbose_eval=200\n",
    "        )\n",
    "\n",
    "        # Validation 데이터 예측\n",
    "        val_preds = clf.predict(x_val)\n",
    "        \n",
    "        # Validation index에 예측값 저장 \n",
    "        y_oof[val_idx] = val_preds\n",
    "        \n",
    "        # 폴드별 Validation 스코어 측정\n",
    "        print(f\"Fold {fold + 1} | AUC: {roc_auc_score(y_val, val_preds)}\")\n",
    "        print('-'*80)\n",
    "\n",
    "        # score 변수에 폴드별 평균 Validation 스코어 저장\n",
    "        score += roc_auc_score(y_val, val_preds) / folds\n",
    "        \n",
    "        # 테스트 데이터 예측하고 평균해서 저장\n",
    "        test_preds += clf.predict(x_test) / folds\n",
    "        \n",
    "        # 폴드별 피처 중요도 저장\n",
    "        fi[f'fold_{fold+1}'] = clf.feature_importance()\n",
    "\n",
    "        del x_tr, x_val, y_tr, y_val\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f\"\\nMean AUC = {score}\") # 폴드별 Validation 스코어 출력\n",
    "    print(f\"OOF AUC = {roc_auc_score(y, y_oof)}\") # Out Of Fold Validation 스코어 출력\n",
    "        \n",
    "    # 폴드별 피처 중요도 평균값 계산해서 저장 \n",
    "    fi_cols = [col for col in fi.columns if 'fold_' in col]\n",
    "    fi['importance'] = fi[fi_cols].mean(axis=1)\n",
    "    \n",
    "    return y_oof, test_preds, fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c2e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n=20, color='blue', figsize=(12,8)):\n",
    "    # 피처 중요도 순으로 내림차순 정렬\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # 피처 중요도 정규화 및 누적 중요도 계산\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    # 피처 중요도 순으로 n개까지 바플롯으로 그리기\n",
    "    df.loc[:n, :].plot.barh(y='importance_normalized', \n",
    "                            x='feature', color=color, \n",
    "                            edgecolor='k', figsize=figsize,\n",
    "                            legend=False)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size=18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size=18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aad9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_2011_11 = generate_label(data, '2011-11')['label']\n",
    "\n",
    "\n",
    "# # ## Label 데이터 분포 플롯\n",
    "# # sns.countplot(label_2011_11);\n",
    "# # label_2011_11.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f9bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'objective': 'binary', # 이진 분류\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'auc', # 평가 지표 설정\n",
    "    'feature_fraction': 0.8, # 피처 샘플링 비율\n",
    "    'bagging_fraction': 0.8, # 데이터 샘플링 비율\n",
    "    'bagging_freq': 1,\n",
    "    'n_estimators': 10000, # 트리 개수(반복횟수)\n",
    "    'early_stopping_rounds': 100,\n",
    "    'seed': SEED,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a02c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, y, features = feature_engineering2(data, '2011-11')\n",
    "\n",
    "# y_oof, test_preds_2011_11, fi = make_lgb_oof_prediction(train, y, test, features, model_params=model_params)\n",
    "# print_score(label_2011_11, test_preds_2011_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b79461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380,)\n",
      "order_ts-first\n",
      "order_ts-last\n",
      "order_ts_diff-mean\n",
      "order_ts_diff-max\n",
      "order_ts_diff-min\n",
      "order_ts_diff-sum\n",
      "order_ts_diff-count\n",
      "order_ts_diff-std\n",
      "order_ts_diff-skew\n",
      "order_ts_plus-first\n",
      "order_ts_plus-last\n",
      "order_ts_plus_diff-mean\n",
      "order_ts_plus_diff-max\n",
      "order_ts_plus_diff-min\n",
      "order_ts_plus_diff-sum\n",
      "order_ts_plus_diff-count\n",
      "order_ts_plus_diff-std\n",
      "order_ts_plus_diff-skew\n",
      "quantity_diff-mean\n",
      "quantity_diff-max\n",
      "quantity_diff-min\n",
      "quantity_diff-sum\n",
      "quantity_diff-count\n",
      "quantity_diff-std\n",
      "quantity_diff-skew\n",
      "price_diff-mean\n",
      "price_diff-max\n",
      "price_diff-min\n",
      "price_diff-sum\n",
      "price_diff-count\n",
      "price_diff-std\n",
      "price_diff-skew\n",
      "total_diff-mean\n",
      "total_diff-max\n",
      "total_diff-min\n",
      "total_diff-sum\n",
      "total_diff-count\n",
      "total_diff-std\n",
      "total_diff-skew\n",
      "quantity-mean\n",
      "quantity-max\n",
      "quantity-min\n",
      "quantity-sum\n",
      "quantity-count\n",
      "quantity-std\n",
      "quantity-skew\n",
      "price-mean\n",
      "price-max\n",
      "price-min\n",
      "price-sum\n",
      "price-count\n",
      "price-std\n",
      "price-skew\n",
      "total-mean\n",
      "total-max\n",
      "total-min\n",
      "total-sum\n",
      "total-count\n",
      "total-std\n",
      "total-skew\n",
      "cumsum_total_by_cust_id-mean\n",
      "cumsum_total_by_cust_id-max\n",
      "cumsum_total_by_cust_id-min\n",
      "cumsum_total_by_cust_id-sum\n",
      "cumsum_total_by_cust_id-count\n",
      "cumsum_total_by_cust_id-std\n",
      "cumsum_total_by_cust_id-skew\n",
      "cumsum_quantity_by_cust_id-mean\n",
      "cumsum_quantity_by_cust_id-max\n",
      "cumsum_quantity_by_cust_id-min\n",
      "cumsum_quantity_by_cust_id-sum\n",
      "cumsum_quantity_by_cust_id-count\n",
      "cumsum_quantity_by_cust_id-std\n",
      "cumsum_quantity_by_cust_id-skew\n",
      "cumsum_price_by_cust_id-mean\n",
      "cumsum_price_by_cust_id-max\n",
      "cumsum_price_by_cust_id-min\n",
      "cumsum_price_by_cust_id-sum\n",
      "cumsum_price_by_cust_id-count\n",
      "cumsum_price_by_cust_id-std\n",
      "cumsum_price_by_cust_id-skew\n",
      "cumsum_total_by_prod_id-mean\n",
      "cumsum_total_by_prod_id-max\n",
      "cumsum_total_by_prod_id-min\n",
      "cumsum_total_by_prod_id-sum\n",
      "cumsum_total_by_prod_id-count\n",
      "cumsum_total_by_prod_id-std\n",
      "cumsum_total_by_prod_id-skew\n",
      "cumsum_quantity_by_prod_id-mean\n",
      "cumsum_quantity_by_prod_id-max\n",
      "cumsum_quantity_by_prod_id-min\n",
      "cumsum_quantity_by_prod_id-sum\n",
      "cumsum_quantity_by_prod_id-count\n",
      "cumsum_quantity_by_prod_id-std\n",
      "cumsum_quantity_by_prod_id-skew\n",
      "cumsum_price_by_prod_id-mean\n",
      "cumsum_price_by_prod_id-max\n",
      "cumsum_price_by_prod_id-min\n",
      "cumsum_price_by_prod_id-sum\n",
      "cumsum_price_by_prod_id-count\n",
      "cumsum_price_by_prod_id-std\n",
      "cumsum_price_by_prod_id-skew\n",
      "cumsum_total_by_order_id-mean\n",
      "cumsum_total_by_order_id-max\n",
      "cumsum_total_by_order_id-min\n",
      "cumsum_total_by_order_id-sum\n",
      "cumsum_total_by_order_id-count\n",
      "cumsum_total_by_order_id-std\n",
      "cumsum_total_by_order_id-skew\n",
      "cumsum_quantity_by_order_id-mean\n",
      "cumsum_quantity_by_order_id-max\n",
      "cumsum_quantity_by_order_id-min\n",
      "cumsum_quantity_by_order_id-sum\n",
      "cumsum_quantity_by_order_id-count\n",
      "cumsum_quantity_by_order_id-std\n",
      "cumsum_quantity_by_order_id-skew\n",
      "cumsum_price_by_order_id-mean\n",
      "cumsum_price_by_order_id-max\n",
      "cumsum_price_by_order_id-min\n",
      "cumsum_price_by_order_id-sum\n",
      "cumsum_price_by_order_id-count\n",
      "cumsum_price_by_order_id-std\n",
      "cumsum_price_by_order_id-skew\n",
      "order_id-nunique\n",
      "product_id-nunique\n",
      "month-mode-mode\n",
      "year_month-mode-mode\n",
      "cycle_1224\n",
      "quantity-max-0\n",
      "quantity-min-0\n",
      "quantity-sum-0\n",
      "quantity-mean-0\n",
      "quantity-count-0\n",
      "quantity-std-0\n",
      "quantity-skew-0\n",
      "price-max-0\n",
      "price-min-0\n",
      "price-sum-0\n",
      "price-mean-0\n",
      "price-count-0\n",
      "price-std-0\n",
      "price-skew-0\n",
      "total-max-0\n",
      "total-min-0\n",
      "total-sum-0\n",
      "total-mean-0\n",
      "total-count-0\n",
      "total-std-0\n",
      "total-skew-0\n",
      "quantity-max-1\n",
      "quantity-min-1\n",
      "quantity-sum-1\n",
      "quantity-mean-1\n",
      "quantity-count-1\n",
      "quantity-std-1\n",
      "quantity-skew-1\n",
      "price-max-1\n",
      "price-min-1\n",
      "price-sum-1\n",
      "price-mean-1\n",
      "price-count-1\n",
      "price-std-1\n",
      "price-skew-1\n",
      "total-max-1\n",
      "total-min-1\n",
      "total-sum-1\n",
      "total-mean-1\n",
      "total-count-1\n",
      "total-std-1\n",
      "total-skew-1\n",
      "quantity-max-2\n",
      "quantity-min-2\n",
      "quantity-sum-2\n",
      "quantity-mean-2\n",
      "quantity-count-2\n",
      "quantity-std-2\n",
      "quantity-skew-2\n",
      "price-max-2\n",
      "price-min-2\n",
      "price-sum-2\n",
      "price-mean-2\n",
      "price-count-2\n",
      "price-std-2\n",
      "price-skew-2\n",
      "total-max-2\n",
      "total-min-2\n",
      "total-sum-2\n",
      "total-mean-2\n",
      "total-count-2\n",
      "total-std-2\n",
      "total-skew-2\n",
      "quantity-max-3\n",
      "quantity-min-3\n",
      "quantity-sum-3\n",
      "quantity-mean-3\n",
      "quantity-count-3\n",
      "quantity-std-3\n",
      "quantity-skew-3\n",
      "price-max-3\n",
      "price-min-3\n",
      "price-sum-3\n",
      "price-mean-3\n",
      "price-count-3\n",
      "price-std-3\n",
      "price-skew-3\n",
      "total-max-3\n",
      "total-min-3\n",
      "total-sum-3\n",
      "total-mean-3\n",
      "total-count-3\n",
      "total-std-3\n",
      "total-skew-3\n",
      "quantity-max-4\n",
      "quantity-min-4\n",
      "quantity-sum-4\n",
      "quantity-mean-4\n",
      "quantity-count-4\n",
      "quantity-std-4\n",
      "quantity-skew-4\n",
      "price-max-4\n",
      "price-min-4\n",
      "price-sum-4\n",
      "price-mean-4\n",
      "price-count-4\n",
      "price-std-4\n",
      "price-skew-4\n",
      "total-max-4\n",
      "total-min-4\n",
      "total-sum-4\n",
      "total-mean-4\n",
      "total-count-4\n",
      "total-std-4\n",
      "total-skew-4\n",
      "quantity-max-5\n",
      "quantity-min-5\n",
      "quantity-sum-5\n",
      "quantity-mean-5\n",
      "quantity-count-5\n",
      "quantity-std-5\n",
      "quantity-skew-5\n",
      "price-max-5\n",
      "price-min-5\n",
      "price-sum-5\n",
      "price-mean-5\n",
      "price-count-5\n",
      "price-std-5\n",
      "price-skew-5\n",
      "total-max-5\n",
      "total-min-5\n",
      "total-sum-5\n",
      "total-mean-5\n",
      "total-count-5\n",
      "total-std-5\n",
      "total-skew-5\n",
      "quantity-max-6\n",
      "quantity-min-6\n",
      "quantity-sum-6\n",
      "quantity-mean-6\n",
      "quantity-count-6\n",
      "quantity-std-6\n",
      "quantity-skew-6\n",
      "price-max-6\n",
      "price-min-6\n",
      "price-sum-6\n",
      "price-mean-6\n",
      "price-count-6\n",
      "price-std-6\n",
      "price-skew-6\n",
      "total-max-6\n",
      "total-min-6\n",
      "total-sum-6\n",
      "total-mean-6\n",
      "total-count-6\n",
      "total-std-6\n",
      "total-skew-6\n",
      "quantity-max-7\n",
      "quantity-min-7\n",
      "quantity-sum-7\n",
      "quantity-mean-7\n",
      "quantity-count-7\n",
      "quantity-std-7\n",
      "quantity-skew-7\n",
      "price-max-7\n",
      "price-min-7\n",
      "price-sum-7\n",
      "price-mean-7\n",
      "price-count-7\n",
      "price-std-7\n",
      "price-skew-7\n",
      "total-max-7\n",
      "total-min-7\n",
      "total-sum-7\n",
      "total-mean-7\n",
      "total-count-7\n",
      "total-std-7\n",
      "total-skew-7\n",
      "quantity-max-season0\n",
      "quantity-min-season0\n",
      "quantity-sum-season0\n",
      "quantity-mean-season0\n",
      "quantity-count-season0\n",
      "quantity-std-season0\n",
      "quantity-skew-season0\n",
      "price-max-season0\n",
      "price-min-season0\n",
      "price-sum-season0\n",
      "price-mean-season0\n",
      "price-count-season0\n",
      "price-std-season0\n",
      "price-skew-season0\n",
      "total-max-season0\n",
      "total-min-season0\n",
      "total-sum-season0\n",
      "total-mean-season0\n",
      "total-count-season0\n",
      "total-std-season0\n",
      "total-skew-season0\n",
      "quantity-max-season1\n",
      "quantity-min-season1\n",
      "quantity-sum-season1\n",
      "quantity-mean-season1\n",
      "quantity-count-season1\n",
      "quantity-std-season1\n",
      "quantity-skew-season1\n",
      "price-max-season1\n",
      "price-min-season1\n",
      "price-sum-season1\n",
      "price-mean-season1\n",
      "price-count-season1\n",
      "price-std-season1\n",
      "price-skew-season1\n",
      "total-max-season1\n",
      "total-min-season1\n",
      "total-sum-season1\n",
      "total-mean-season1\n",
      "total-count-season1\n",
      "total-std-season1\n",
      "total-skew-season1\n",
      "quantity-max-season2\n",
      "quantity-min-season2\n",
      "quantity-sum-season2\n",
      "quantity-mean-season2\n",
      "quantity-count-season2\n",
      "quantity-std-season2\n",
      "quantity-skew-season2\n",
      "price-max-season2\n",
      "price-min-season2\n",
      "price-sum-season2\n",
      "price-mean-season2\n",
      "price-count-season2\n",
      "price-std-season2\n",
      "price-skew-season2\n",
      "total-max-season2\n",
      "total-min-season2\n",
      "total-sum-season2\n",
      "total-mean-season2\n",
      "total-count-season2\n",
      "total-std-season2\n",
      "total-skew-season2\n",
      "quantity-max-season3\n",
      "quantity-min-season3\n",
      "quantity-sum-season3\n",
      "quantity-mean-season3\n",
      "quantity-count-season3\n",
      "quantity-std-season3\n",
      "quantity-skew-season3\n",
      "price-max-season3\n",
      "price-min-season3\n",
      "price-sum-season3\n",
      "price-mean-season3\n",
      "price-count-season3\n",
      "price-std-season3\n",
      "price-skew-season3\n",
      "total-max-season3\n",
      "total-min-season3\n",
      "total-sum-season3\n",
      "total-mean-season3\n",
      "total-count-season3\n",
      "total-std-season3\n",
      "total-skew-season3\n",
      "(5722, 383)\n",
      "(5914, 383)\n",
      "categorical feature: ['year_month-mode-mode']\n",
      "x_tr.shape (5722, 383) , x_te.shape (5914, 383)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_dir + '/train.csv', parse_dates=['order_date'])\n",
    "train, test, y, features = feature_engineering2(data, '2011-12')\n",
    "# y_oof, test_preds, fi = make_lgb_oof_prediction(train, y, test, features, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59bb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantile 실험\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# quan = QuantileTransformer(n_quantiles=2000, output_distribution='normal',random_state=42)\n",
    "quan = QuantileTransformer(n_quantiles=1000, random_state=42)\n",
    "X_quan = quan.fit_transform(train[features])\n",
    "Y_quan = quan.fit_transform(test[features])\n",
    "x_quan = pd.DataFrame(X_quan, columns=features)\n",
    "y_quan = pd.DataFrame(Y_quan, columns=features)\n",
    "train[features] = x_quan[features]\n",
    "test[features] = y_quan[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28ea77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "814343b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = TabNetClassifier(\n",
    "#     optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2),\n",
    "#     scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "#                       \"gamma\":0.9},\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "#     mask_type='sparsemax' # This will be overwritten if using pretrain model\n",
    "# )\n",
    "\n",
    "# clf.fit(\n",
    "#     X_train=train[features].values, y_train=y,\n",
    "#     eval_set= [(train[features].values, y)],\n",
    "#     eval_name=['train'], #, 'valid'],\n",
    "#     eval_metric=['auc'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "310acc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 761507.50805|  0:00:00s\n",
      "epoch 1  | loss: 150351.20642|  0:00:01s\n",
      "epoch 2  | loss: 45443.30082|  0:00:02s\n",
      "epoch 3  | loss: 12429.08759|  0:00:02s\n",
      "epoch 4  | loss: 2929.05939|  0:00:03s\n",
      "epoch 5  | loss: 479.02132|  0:00:04s\n",
      "epoch 6  | loss: 109.1671|  0:00:04s\n",
      "epoch 7  | loss: 29.0335 |  0:00:05s\n",
      "epoch 8  | loss: 31.49836|  0:00:05s\n",
      "epoch 9  | loss: 17.04393|  0:00:06s\n",
      "epoch 10 | loss: 6.89258 |  0:00:06s\n",
      "epoch 11 | loss: 7.76593 |  0:00:07s\n",
      "epoch 12 | loss: 8.30071 |  0:00:07s\n",
      "epoch 13 | loss: 5.47485 |  0:00:08s\n",
      "epoch 14 | loss: 4.33075 |  0:00:09s\n",
      "epoch 15 | loss: 3.9328  |  0:00:09s\n",
      "epoch 16 | loss: 3.64508 |  0:00:10s\n",
      "epoch 17 | loss: 3.57547 |  0:00:10s\n",
      "epoch 18 | loss: 3.3733  |  0:00:11s\n",
      "epoch 19 | loss: 3.24925 |  0:00:11s\n",
      "epoch 20 | loss: 2.92094 |  0:00:12s\n",
      "epoch 21 | loss: 2.74688 |  0:00:13s\n",
      "epoch 22 | loss: 2.40697 |  0:00:13s\n",
      "epoch 23 | loss: 2.15205 |  0:00:14s\n",
      "epoch 24 | loss: 2.20258 |  0:00:14s\n",
      "epoch 25 | loss: 1.99557 |  0:00:15s\n",
      "epoch 26 | loss: 1.88452 |  0:00:15s\n",
      "epoch 27 | loss: 1.84696 |  0:00:16s\n",
      "epoch 28 | loss: 1.83626 |  0:00:16s\n",
      "epoch 29 | loss: 1.68581 |  0:00:17s\n",
      "epoch 30 | loss: 1.73267 |  0:00:18s\n",
      "epoch 31 | loss: 1.62355 |  0:00:18s\n",
      "epoch 32 | loss: 1.59282 |  0:00:19s\n",
      "epoch 33 | loss: 1.59871 |  0:00:19s\n",
      "epoch 34 | loss: 1.55    |  0:00:20s\n",
      "epoch 35 | loss: 1.44765 |  0:00:20s\n",
      "epoch 36 | loss: 1.51564 |  0:00:21s\n",
      "epoch 37 | loss: 1.46263 |  0:00:21s\n",
      "epoch 38 | loss: 1.39177 |  0:00:22s\n",
      "epoch 39 | loss: 1.42468 |  0:00:22s\n",
      "epoch 40 | loss: 1.35146 |  0:00:23s\n",
      "epoch 41 | loss: 1.39154 |  0:00:24s\n",
      "epoch 42 | loss: 1.36637 |  0:00:24s\n",
      "epoch 43 | loss: 1.31079 |  0:00:25s\n",
      "epoch 44 | loss: 1.33634 |  0:00:25s\n",
      "epoch 45 | loss: 1.34751 |  0:00:26s\n",
      "epoch 46 | loss: 1.29388 |  0:00:26s\n",
      "epoch 47 | loss: 1.27735 |  0:00:27s\n",
      "epoch 48 | loss: 1.23472 |  0:00:27s\n",
      "epoch 49 | loss: 1.235   |  0:00:28s\n",
      "epoch 50 | loss: 1.26429 |  0:00:28s\n",
      "epoch 51 | loss: 1.18159 |  0:00:29s\n",
      "epoch 52 | loss: 1.2088  |  0:00:29s\n",
      "epoch 53 | loss: 1.17457 |  0:00:30s\n",
      "epoch 54 | loss: 1.12846 |  0:00:30s\n",
      "epoch 55 | loss: 1.14687 |  0:00:31s\n",
      "epoch 56 | loss: 1.12774 |  0:00:31s\n",
      "epoch 57 | loss: 1.12148 |  0:00:32s\n",
      "epoch 58 | loss: 1.13194 |  0:00:32s\n",
      "epoch 59 | loss: 1.11816 |  0:00:33s\n",
      "epoch 60 | loss: 1.08155 |  0:00:33s\n",
      "epoch 61 | loss: 1.11305 |  0:00:34s\n",
      "epoch 62 | loss: 1.10747 |  0:00:34s\n",
      "epoch 63 | loss: 1.08358 |  0:00:35s\n",
      "epoch 64 | loss: 1.09105 |  0:00:35s\n",
      "epoch 65 | loss: 1.0625  |  0:00:36s\n",
      "epoch 66 | loss: 1.06405 |  0:00:37s\n",
      "epoch 67 | loss: 1.068   |  0:00:37s\n",
      "epoch 68 | loss: 1.03917 |  0:00:38s\n",
      "epoch 69 | loss: 1.03732 |  0:00:38s\n",
      "epoch 70 | loss: 1.02952 |  0:00:39s\n",
      "epoch 71 | loss: 1.0337  |  0:00:39s\n",
      "epoch 72 | loss: 1.02305 |  0:00:40s\n",
      "epoch 73 | loss: 1.02398 |  0:00:40s\n",
      "epoch 74 | loss: 1.04921 |  0:00:41s\n",
      "epoch 75 | loss: 1.0026  |  0:00:41s\n",
      "epoch 76 | loss: 1.02219 |  0:00:42s\n",
      "epoch 77 | loss: 0.99912 |  0:00:42s\n",
      "epoch 78 | loss: 0.99294 |  0:00:43s\n",
      "epoch 79 | loss: 0.99993 |  0:00:43s\n",
      "epoch 80 | loss: 1.07243 |  0:00:44s\n",
      "epoch 81 | loss: 1.41182 |  0:00:45s\n",
      "epoch 82 | loss: 3.7507  |  0:00:45s\n",
      "epoch 83 | loss: 1.42472 |  0:00:46s\n",
      "epoch 84 | loss: 12.31116|  0:00:46s\n",
      "epoch 85 | loss: 1.32526 |  0:00:47s\n",
      "epoch 86 | loss: 1.40554 |  0:00:47s\n",
      "epoch 87 | loss: 1.61456 |  0:00:48s\n",
      "epoch 88 | loss: 1.18321 |  0:00:48s\n",
      "epoch 89 | loss: 1.32831 |  0:00:49s\n",
      "epoch 90 | loss: 1.35007 |  0:00:49s\n",
      "epoch 91 | loss: 1.45595 |  0:00:50s\n",
      "epoch 92 | loss: 1.54497 |  0:00:50s\n",
      "epoch 93 | loss: 1.04472 |  0:00:51s\n",
      "epoch 94 | loss: 1.93313 |  0:00:51s\n",
      "epoch 95 | loss: 3.34485 |  0:00:52s\n",
      "epoch 96 | loss: 1.24546 |  0:00:52s\n",
      "epoch 97 | loss: 2.2434  |  0:00:53s\n",
      "epoch 98 | loss: 2.06815 |  0:00:53s\n",
      "epoch 99 | loss: 1.2461  |  0:00:54s\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 0.7093  | train_auc: 0.44858 |  0:00:00s\n",
      "epoch 1  | loss: 0.68996 | train_auc: 0.62969 |  0:00:01s\n",
      "epoch 2  | loss: 0.67355 | train_auc: 0.62337 |  0:00:01s\n",
      "epoch 3  | loss: 0.65976 | train_auc: 0.62813 |  0:00:02s\n",
      "epoch 4  | loss: 0.6563  | train_auc: 0.63385 |  0:00:02s\n",
      "epoch 5  | loss: 0.64868 | train_auc: 0.67647 |  0:00:03s\n",
      "epoch 6  | loss: 0.60644 | train_auc: 0.69907 |  0:00:03s\n",
      "epoch 7  | loss: 0.52176 | train_auc: 0.69878 |  0:00:04s\n",
      "epoch 8  | loss: 0.471   | train_auc: 0.71627 |  0:00:04s\n",
      "epoch 9  | loss: 0.43187 | train_auc: 0.74456 |  0:00:05s\n",
      "epoch 10 | loss: 0.428   | train_auc: 0.75527 |  0:00:05s\n",
      "epoch 11 | loss: 0.41465 | train_auc: 0.78652 |  0:00:06s\n",
      "epoch 12 | loss: 0.41376 | train_auc: 0.78401 |  0:00:06s\n",
      "epoch 13 | loss: 0.40793 | train_auc: 0.79191 |  0:00:07s\n",
      "epoch 14 | loss: 0.40589 | train_auc: 0.79912 |  0:00:07s\n",
      "epoch 15 | loss: 0.40608 | train_auc: 0.80439 |  0:00:08s\n",
      "epoch 16 | loss: 0.40055 | train_auc: 0.80674 |  0:00:08s\n",
      "epoch 17 | loss: 0.40826 | train_auc: 0.80489 |  0:00:09s\n",
      "epoch 18 | loss: 0.40276 | train_auc: 0.80633 |  0:00:09s\n",
      "epoch 19 | loss: 0.39797 | train_auc: 0.80334 |  0:00:10s\n",
      "epoch 20 | loss: 0.39632 | train_auc: 0.80882 |  0:00:11s\n",
      "epoch 21 | loss: 0.39342 | train_auc: 0.81453 |  0:00:11s\n",
      "epoch 22 | loss: 0.39541 | train_auc: 0.81793 |  0:00:12s\n",
      "epoch 23 | loss: 0.39216 | train_auc: 0.8185  |  0:00:12s\n",
      "epoch 24 | loss: 0.39032 | train_auc: 0.82033 |  0:00:13s\n",
      "epoch 25 | loss: 0.39274 | train_auc: 0.82296 |  0:00:13s\n",
      "epoch 26 | loss: 0.39111 | train_auc: 0.82386 |  0:00:14s\n",
      "epoch 27 | loss: 0.38882 | train_auc: 0.82387 |  0:00:14s\n",
      "epoch 28 | loss: 0.3911  | train_auc: 0.82644 |  0:00:15s\n",
      "epoch 29 | loss: 0.38799 | train_auc: 0.82655 |  0:00:15s\n",
      "epoch 30 | loss: 0.38454 | train_auc: 0.82477 |  0:00:16s\n",
      "epoch 31 | loss: 0.38422 | train_auc: 0.82739 |  0:00:16s\n",
      "epoch 32 | loss: 0.38603 | train_auc: 0.83074 |  0:00:17s\n",
      "epoch 33 | loss: 0.38749 | train_auc: 0.82984 |  0:00:17s\n",
      "epoch 34 | loss: 0.3872  | train_auc: 0.83039 |  0:00:18s\n",
      "epoch 35 | loss: 0.38655 | train_auc: 0.83276 |  0:00:18s\n",
      "epoch 36 | loss: 0.38389 | train_auc: 0.83586 |  0:00:19s\n",
      "epoch 37 | loss: 0.38437 | train_auc: 0.83416 |  0:00:20s\n",
      "epoch 38 | loss: 0.38477 | train_auc: 0.83479 |  0:00:20s\n",
      "epoch 39 | loss: 0.38256 | train_auc: 0.8335  |  0:00:21s\n",
      "epoch 40 | loss: 0.3808  | train_auc: 0.83921 |  0:00:21s\n",
      "epoch 41 | loss: 0.37781 | train_auc: 0.84172 |  0:00:22s\n",
      "epoch 42 | loss: 0.38032 | train_auc: 0.84556 |  0:00:22s\n",
      "epoch 43 | loss: 0.37586 | train_auc: 0.84266 |  0:00:23s\n",
      "epoch 44 | loss: 0.37771 | train_auc: 0.83817 |  0:00:23s\n",
      "epoch 45 | loss: 0.37424 | train_auc: 0.84149 |  0:00:24s\n",
      "epoch 46 | loss: 0.37306 | train_auc: 0.84579 |  0:00:24s\n",
      "epoch 47 | loss: 0.37756 | train_auc: 0.84623 |  0:00:25s\n",
      "epoch 48 | loss: 0.37812 | train_auc: 0.846   |  0:00:25s\n",
      "epoch 49 | loss: 0.3748  | train_auc: 0.85087 |  0:00:26s\n",
      "epoch 50 | loss: 0.37139 | train_auc: 0.85111 |  0:00:26s\n",
      "epoch 51 | loss: 0.37446 | train_auc: 0.85003 |  0:00:27s\n",
      "epoch 52 | loss: 0.37206 | train_auc: 0.84961 |  0:00:27s\n",
      "epoch 53 | loss: 0.36986 | train_auc: 0.85381 |  0:00:28s\n",
      "epoch 54 | loss: 0.36837 | train_auc: 0.85699 |  0:00:28s\n",
      "epoch 55 | loss: 0.36947 | train_auc: 0.86111 |  0:00:29s\n",
      "epoch 56 | loss: 0.36688 | train_auc: 0.85507 |  0:00:29s\n",
      "epoch 57 | loss: 0.3671  | train_auc: 0.85816 |  0:00:30s\n",
      "epoch 58 | loss: 0.36342 | train_auc: 0.85541 |  0:00:30s\n",
      "epoch 59 | loss: 0.36185 | train_auc: 0.86036 |  0:00:31s\n",
      "epoch 60 | loss: 0.36258 | train_auc: 0.86143 |  0:00:31s\n",
      "epoch 61 | loss: 0.36049 | train_auc: 0.86282 |  0:00:32s\n",
      "epoch 62 | loss: 0.3618  | train_auc: 0.86643 |  0:00:32s\n",
      "epoch 63 | loss: 0.35453 | train_auc: 0.86924 |  0:00:33s\n",
      "epoch 64 | loss: 0.35526 | train_auc: 0.87198 |  0:00:34s\n",
      "epoch 65 | loss: 0.35504 | train_auc: 0.87242 |  0:00:34s\n",
      "epoch 66 | loss: 0.355   | train_auc: 0.87095 |  0:00:35s\n",
      "epoch 67 | loss: 0.35319 | train_auc: 0.87528 |  0:00:35s\n",
      "epoch 68 | loss: 0.35207 | train_auc: 0.87438 |  0:00:36s\n",
      "epoch 69 | loss: 0.35613 | train_auc: 0.87361 |  0:00:36s\n",
      "epoch 70 | loss: 0.3536  | train_auc: 0.87367 |  0:00:37s\n",
      "epoch 71 | loss: 0.35379 | train_auc: 0.86818 |  0:00:37s\n",
      "epoch 72 | loss: 0.36015 | train_auc: 0.87259 |  0:00:38s\n",
      "epoch 73 | loss: 0.35135 | train_auc: 0.87528 |  0:00:38s\n",
      "epoch 74 | loss: 0.35014 | train_auc: 0.87564 |  0:00:39s\n",
      "epoch 75 | loss: 0.34785 | train_auc: 0.87695 |  0:00:39s\n",
      "epoch 76 | loss: 0.34611 | train_auc: 0.87682 |  0:00:40s\n",
      "epoch 77 | loss: 0.34705 | train_auc: 0.88063 |  0:00:40s\n",
      "epoch 78 | loss: 0.34486 | train_auc: 0.88139 |  0:00:41s\n",
      "epoch 79 | loss: 0.3465  | train_auc: 0.88461 |  0:00:41s\n",
      "epoch 80 | loss: 0.34219 | train_auc: 0.88538 |  0:00:42s\n",
      "epoch 81 | loss: 0.34019 | train_auc: 0.88433 |  0:00:42s\n",
      "epoch 82 | loss: 0.34747 | train_auc: 0.88484 |  0:00:43s\n",
      "epoch 83 | loss: 0.34337 | train_auc: 0.88596 |  0:00:43s\n",
      "epoch 84 | loss: 0.33899 | train_auc: 0.88255 |  0:00:44s\n",
      "epoch 85 | loss: 0.33542 | train_auc: 0.88465 |  0:00:44s\n",
      "epoch 86 | loss: 0.33869 | train_auc: 0.88772 |  0:00:45s\n",
      "epoch 87 | loss: 0.33825 | train_auc: 0.89123 |  0:00:45s\n",
      "epoch 88 | loss: 0.33253 | train_auc: 0.8905  |  0:00:46s\n",
      "epoch 89 | loss: 0.3352  | train_auc: 0.89335 |  0:00:46s\n",
      "epoch 90 | loss: 0.33234 | train_auc: 0.8906  |  0:00:47s\n",
      "epoch 91 | loss: 0.33105 | train_auc: 0.89242 |  0:00:48s\n",
      "epoch 92 | loss: 0.32746 | train_auc: 0.89607 |  0:00:48s\n",
      "epoch 93 | loss: 0.32829 | train_auc: 0.89486 |  0:00:49s\n",
      "epoch 94 | loss: 0.32508 | train_auc: 0.89763 |  0:00:49s\n",
      "epoch 95 | loss: 0.3296  | train_auc: 0.89669 |  0:00:50s\n",
      "epoch 96 | loss: 0.33235 | train_auc: 0.89796 |  0:00:50s\n",
      "epoch 97 | loss: 0.3234  | train_auc: 0.8995  |  0:00:51s\n",
      "epoch 98 | loss: 0.3216  | train_auc: 0.89953 |  0:00:51s\n",
      "epoch 99 | loss: 0.32484 | train_auc: 0.90068 |  0:00:52s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_train_auc = 0.90068\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    optimizer_params = dict(lr=2e-2),\n",
    "    mask_type='sparsemax'\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train = train[features].values,\n",
    "    pretraining_ratio=0.8,\n",
    ")\n",
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                      \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax' # This will be overwritten if using pretrain model\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train=train[features].values, y_train=y,\n",
    "    eval_set= [(train[features].values, y)],\n",
    "    eval_name=['train'], #, 'valid'],\n",
    "    eval_metric=['auc'],\n",
    "    from_unsupervised = unsupervised_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48e90352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(test[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feadc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3299d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830276b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['probability'] = pred\n",
    "submission.to_csv(os.path.join(output_dir, 'output20.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedba719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcafb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db1e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
